\documentclass[11pt,a4paper]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{enumitem}

\geometry{margin=2.5cm}

\title{Book Recommendation System Using Distributed Item--Item Collaborative Filtering}
\author{Vadim Sokolov \\ Master's Degree in Computer Science}
\date{Algorithms for Massive Datasets -- Project}

\begin{document}

\maketitle

\section{Introduction}

This project implements a book recommendation system using large-scale user review data. The system is designed following techniques studied in the course, in particular collaborative filtering implemented on distributed data processing frameworks.

The goal is to recommend books to users based on past ratings while handling datasets that cannot be processed efficiently on a single machine without distributed techniques.

The system is implemented in Google Colab using PySpark and processes Amazon Books Reviews data from Kaggle.

\section{Dataset}

We use the \textit{Amazon Books Reviews} dataset available on Kaggle. It contains user reviews and ratings for books.

Each rating record contains:

\begin{itemize}
\item user identifier
\item book identifier
\item rating score (1--5)
\item book title
\end{itemize}

After cleaning invalid ratings and removing missing values, we randomly sample 20\% of the dataset for efficient experimentation.

After filtering users and books with very few ratings, the resulting dataset contains:

\begin{itemize}
\item 85,603 ratings
\item 9,795 users
\item 13,660 books
\end{itemize}

\section{Method}

We implement an \textbf{item--item collaborative filtering} recommender system.

The pipeline is:

\begin{enumerate}
\item Group ratings by user.
\item Generate pairs of items rated by the same user.
\item Compute cosine similarity between items.
\item Keep top-$K$ neighbors per item.
\item Predict user ratings using weighted averages of similar items.
\end{enumerate}

Similarity between items $i$ and $j$ is computed as cosine similarity:

\[
sim(i,j) =
\frac{\sum_u r_{ui} r_{uj}}
{\sqrt{\sum_u r_{ui}^2} \sqrt{\sum_u r_{uj}^2}}
\]

Predicted rating for user $u$ on item $i$:

\[
\hat r_{ui} =
\mu_u +
\frac{\sum_{j} sim(i,j) (r_{uj} - \mu_u)}
{\sum_j |sim(i,j)|}
\]

where $\mu_u$ is the user mean rating.

This mean-centering reduces user bias effects.

\section{Implementation Details}

Key parameters:

\begin{itemize}
\item Sampling fraction: 20\%
\item Minimum common users per item pair: 2
\item Neighbors per item: $K=30$
\item Maximum items per user considered: 50
\end{itemize}

All processing steps are executed using Spark RDD transformations and aggregations.

\subsection{Relation to MapReduce Processing}

Although the implementation uses Apache Spark, the core computations follow the MapReduce processing model studied in the course.

The recommendation algorithm is implemented using distributed transformations that correspond to Map and Reduce operations:

\begin{itemize}
\item \textbf{User aggregation:} ratings are mapped to $(user, (item, rating))$ pairs and grouped by user, corresponding to a Map followed by a Reduce-by-key operation.

\item \textbf{Item pair generation:} for each user, all combinations of rated items are generated using a flatMap transformation, producing intermediate key-value pairs representing item pairs.

\item \textbf{Pair statistics aggregation:} statistics required for cosine similarity are aggregated using reduceByKey, which corresponds to a distributed Reduce phase.

\item \textbf{Similarity computation:} aggregated statistics are mapped to cosine similarity values for each item pair.

\item \textbf{Neighborhood construction:} item similarities are regrouped by item and only the top-$K$ neighbors are kept, again using distributed grouping and reduction.

\item \textbf{Prediction and evaluation:} predicted ratings and evaluation metrics such as RMSE are computed via distributed map and aggregation operations.
\end{itemize}

Therefore, while Spark abstracts execution details, the implemented pipeline follows the MapReduce paradigm throughout the similarity computation and recommendation stages.

\section{Evaluation}

We split the dataset randomly:

\begin{itemize}
\item 80\% training
\item 20\% testing
\end{itemize}

Evaluation metrics:

\begin{itemize}
\item Root Mean Square Error (RMSE)
\item Coverage (fraction of predictions produced)
\end{itemize}

Results:

\begin{center}
\begin{tabular}{lcc}
\toprule
Method & RMSE & Coverage \\
\midrule
Global mean baseline & 1.037 & 1.00 \\
Item-item CF & 0.422 & 0.375 \\
\bottomrule
\end{tabular}
\end{center}

The collaborative filtering method significantly improves prediction accuracy compared to the baseline.

\subsection{Top-$K$ Recommendation Quality}

To evaluate recommendation quality as a ranking problem, we compute Top-$10$ recommendations per user and measure HitRate@10, Precision@10, Recall@10 and NDCG@10 on held-out test items. Users with at least 3 training items and at least 1 test item are considered. Results are averaged over 1,935 users (a cap of 2,000 users was used for efficiency).

\begin{center}
\begin{tabular}{lcccc}
\toprule
Method & HitRatex10 & Precisionx10 & Recallx10 & NDCGx10 \\
\midrule
Popularity baseline & 0.0440 & 0.0048 & 0.0219 & 0.0117 \\
Item-item CF & 0.3654 & 0.0447 & 0.2494 & 0.1464 \\
\bottomrule
\end{tabular}
\end{center}

\section{Example Recommendations}

For active users, the system recommends books consistent with their preferences, for example classic literature readers receive recommendations for similar classic works.

The system also provides explanations based on similar books previously rated by the user.

\section{Discussion}

The dataset is strongly skewed toward 5-star ratings, making rating prediction less informative in some cases. However, ranking-based recommendations remain meaningful.

Coverage is limited by sparsity, since many books share few common reviewers.

Also item canonicalization by title was tested; however, naive canonicalization may split/merge items incorrectly and reduces overlap, decreasing coverage.
To avoid recommending multiple editions of the same book, we post-process the Top-N list by grouping items via a normalized title key and keeping only one representative per group.

\subsection{Model Improvements}

After implementing the baseline item-based collaborative filtering recommender, several improvements were introduced to enhance prediction quality and recommendation ranking.

\paragraph{Significance-weighted similarities.}
Cosine similarity between items may become unreliable when computed from only a few common users. To mitigate this effect, a significance weighting factor was introduced:

\[
sim'(i,j) = sim(i,j) \cdot \frac{n_{ij}}{n_{ij} + \beta},
\]

where $n_{ij}$ is the number of users who rated both items and $\beta$ is a smoothing parameter. This reduces the influence of item pairs supported by few users while preserving strong similarities backed by sufficient data.

\paragraph{Mean-centered predictions.}
User rating behavior often differs in scale. Therefore, predictions are computed using mean-centered ratings:

\[
\hat r_{u,i} = \mu_u +
\frac{\sum_{j \in I_u} sim'(i,j)\,(r_{u,j}-\mu_u)}
     {\sum_{j \in I_u} |sim'(i,j)|},
\]

where $\mu_u$ is the mean rating of user $u$. This correction improves personalization and reduces bias caused by overly generous or strict raters.

\paragraph{Recommendation evaluation metrics.}
Beyond prediction RMSE, ranking quality was evaluated using Top-$N$ recommendation metrics:

\begin{itemize}
\item HitRate@10
\item Precision@10
\item Recall@10
\item NDCG@10
\end{itemize}

These metrics measure how effectively the recommender retrieves items that users actually liked.

\paragraph{Results improvement.}
The improvements produced measurable gains over both the minimum version and a popularity-based baseline:

\begin{itemize}
\item RMSE improved from approximately 0.42 to 0.41.
\item Prediction coverage increased from about 37\% to nearly 39\%.
\item HitRate@10 improved to over 40\%.
\item Ranking metrics significantly outperformed the popularity baseline.
\end{itemize}

These results confirm that similarity regularization and mean-centered predictions enhance both accuracy and recommendation usefulness.


Future improvements could include:

\begin{itemize}
\item matrix factorization methods
\item hybrid content-based approaches
\item improved similarity regularization
\item better cold-start handling
\end{itemize}

\section{Conclusion}

In this project, a recommender system for books was implemented from scratch using distributed data processing techniques. The system relies on an item-based collaborative filtering approach, where item similarities are computed using cosine similarity over user ratings.

The implementation follows a distributed MapReduce-style pipeline using Apache Spark, allowing the processing of large-scale rating data. Several improvements were introduced beyond a basic recommender, including similarity significance weighting, mean-centered predictions, duplicate title handling, and ranking-based evaluation metrics.

Experimental results show that the proposed system significantly outperforms a simple popularity-based baseline, both in prediction accuracy and ranking quality. In particular, improvements in RMSE, recommendation coverage, and Top-$N$ ranking metrics demonstrate the effectiveness of similarity regularization and user-centered predictions.

The project illustrates how classical recommendation algorithms can be efficiently implemented in distributed environments and highlights the importance of proper evaluation when dealing with large-scale datasets.

Future improvements could include hybrid models combining collaborative and content-based methods, as well as more advanced matrix factorization or embedding techniques.

Overall, the project successfully demonstrates practical application of algorithms and technologies studied in the course for solving real-world large-scale data processing problems.


\end{document}
